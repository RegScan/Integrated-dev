# 内容合规检测系统 - 性能优化补充方案

## 一、性能需求分析

### 1.1 业务场景规模预估
- **网站扫描**：每日处理 10,000+ 个网站
- **流量检测**：实时处理 1Gbps+ 网络流量
- **告警处理**：每秒处理 100+ 告警事件
- **并发用户**：支持 100+ 并发管理用户

### 1.2 性能目标指标
- **响应时间**：API响应时间 < 200ms（95%分位数）
- **吞吐量**：扫描处理能力 > 1000 sites/min
- **可用性**：系统可用性 > 99.9%
- **并发能力**：支持 1000+ 并发连接

## 二、性能优化策略

### 2.1 数据库性能优化

#### MongoDB 优化
```javascript
// 索引优化
db.website_checks.createIndex({ "domain": 1, "timestamp": -1 })
db.website_checks.createIndex({ "compliant": 1, "timestamp": -1 })
db.website_checks.createIndex({ "scan_type": 1 })

// 连接池配置
const mongoOptions = {
  maxPoolSize: 100,
  minPoolSize: 10,
  maxIdleTimeMS: 30000,
  serverSelectionTimeoutMS: 5000,
  socketTimeoutMS: 45000,
  connectTimeoutMS: 10000
}
```

#### Redis 缓存策略
```python
# 缓存配置
CACHE_CONFIG = {
    "default_ttl": 3600,  # 1小时
    "scan_result_ttl": 86400,  # 24小时
    "beian_info_ttl": 604800,  # 7天
    "max_memory": "2gb",
    "max_memory_policy": "allkeys-lru"
}

# 缓存键设计
class CacheKeys:
    SCAN_RESULT = "scan:result:{domain}"
    BEIAN_INFO = "beian:info:{domain}"
    API_RATE_LIMIT = "rate:limit:{api_key}"
    USER_SESSION = "session:{user_id}"
```

### 2.2 异步处理优化

#### Celery 任务队列优化
```python
# Celery 配置优化
CELERY_CONFIG = {
    "broker_url": "amqp://user:pass@rabbitmq:5672//",
    "result_backend": "redis://redis:6379/1",
    "task_serializer": "json",
    "result_serializer": "json",
    "accept_content": ["json"],
    "timezone": "Asia/Shanghai",
    "enable_utc": True,
    "task_routes": {
        "app.tasks.scan_tasks.*": {"queue": "scan_queue"},
        "app.tasks.alert_tasks.*": {"queue": "alert_queue"},
        "app.tasks.report_tasks.*": {"queue": "report_queue"}
    },
    "task_default_queue": "default",
    "task_default_exchange": "default",
    "task_default_routing_key": "default",
    "worker_prefetch_multiplier": 1,
    "worker_max_tasks_per_child": 1000,
    "worker_max_memory_per_child": 200000  # 200MB
}

# 队列配置
QUEUE_CONFIG = {
    "scan_queue": {
        "concurrency": 10,
        "max_tasks_per_child": 100,
        "max_memory_per_child": 200000
    },
    "alert_queue": {
        "concurrency": 5,
        "max_tasks_per_child": 50,
        "max_memory_per_child": 100000
    }
}
```

### 2.3 API 性能优化

#### FastAPI 异步优化
```python
# 异步数据库连接池
from motor.motor_asyncio import AsyncIOMotorClient
from redis.asyncio import Redis

class DatabaseManager:
    def __init__(self):
        self.mongo_client = AsyncIOMotorClient(
            MONGODB_URL,
            maxPoolSize=100,
            minPoolSize=10,
            maxIdleTimeMS=30000
        )
        self.redis_client = Redis.from_url(
            REDIS_URL,
            max_connections=50,
            decode_responses=True
        )
    
    async def get_scan_result(self, domain: str):
        # 先查缓存
        cached = await self.redis_client.get(f"scan:result:{domain}")
        if cached:
            return json.loads(cached)
        
        # 查数据库
        result = await self.mongo_client.website_scanner.scan_results.find_one(
            {"domain": domain},
            sort=[("timestamp", -1)]
        )
        
        # 写入缓存
        if result:
            await self.redis_client.setex(
                f"scan:result:{domain}",
                3600,  # 1小时过期
                json.dumps(result)
            )
        
        return result

# 异步任务处理
@app.post("/api/v1/scan/batch")
async def batch_scan(domains: List[str]):
    # 异步并发处理
    tasks = []
    for domain in domains:
        task = scan_domain.delay(domain)  # Celery 异步任务
        tasks.append(task)
    
    # 等待所有任务完成
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return {"status": "success", "results": results}
```

### 2.4 爬虫性能优化

#### 并发爬虫优化
```python
import asyncio
from playwright.async_api import async_playwright
from concurrent.futures import ThreadPoolExecutor

class AsyncWebsiteCrawler:
    def __init__(self, max_concurrent=10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent)
    
    async def crawl_websites_batch(self, domains: List[str]):
        """并发爬取多个网站"""
        async def crawl_single(domain):
            async with self.semaphore:
                return await self._crawl_website(domain)
        
        tasks = [crawl_single(domain) for domain in domains]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    async def _crawl_website(self, domain: str):
        """异步爬取单个网站"""
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-dev-shm-usage']
                )
                page = await browser.new_page()
                
                # 设置超时和重试
                await page.goto(f"https://{domain}", timeout=10000)
                await page.wait_for_load_state("networkidle", timeout=5000)
                
                # 提取内容
                text_content = await page.inner_text("body")
                image_urls = await page.eval_on_selector_all(
                    "img", "elements => elements.map(el => el.src)"
                )
                
                await browser.close()
                
                return {
                    "domain": domain,
                    "text": text_content,
                    "images": image_urls[:5],  # 限制图片数量
                    "status": "success"
                }
        except Exception as e:
            return {
                "domain": domain,
                "status": "error",
                "error": str(e)
            }
```

### 2.5 负载均衡优化

#### Nginx 负载均衡配置
```nginx
# nginx.conf
upstream api_servers {
    least_conn;  # 最少连接数算法
    server api1:8000 max_fails=3 fail_timeout=30s;
    server api2:8000 max_fails=3 fail_timeout=30s;
    server api3:8000 max_fails=3 fail_timeout=30s;
    keepalive 32;
}

upstream scan_servers {
    ip_hash;  # 基于IP的哈希算法，保证同一IP请求到同一服务器
    server scan1:8001 max_fails=3 fail_timeout=30s;
    server scan2:8001 max_fails=3 fail_timeout=30s;
    server scan3:8001 max_fails=3 fail_timeout=30s;
    keepalive 32;
}

server {
    listen 80;
    server_name api.example.com;
    
    # 限流配置
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    limit_req zone=api_limit burst=20 nodelay;
    
    location /api/ {
        proxy_pass http://api_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 超时配置
        proxy_connect_timeout 5s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # 缓冲配置
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
    }
}
```

## 三、性能监控和调优

### 3.1 性能监控指标

#### 应用层监控
```python
# Prometheus 指标收集
from prometheus_client import Counter, Histogram, Gauge
import time

# 自定义指标
SCAN_REQUESTS = Counter('scan_requests_total', 'Total scan requests')
SCAN_DURATION = Histogram('scan_duration_seconds', 'Scan duration in seconds')
ACTIVE_SCANS = Gauge('active_scans', 'Number of active scans')
SCAN_SUCCESS_RATE = Gauge('scan_success_rate', 'Scan success rate')

@app.middleware("http")
async def performance_middleware(request: Request, call_next):
    start_time = time.time()
    
    # 记录请求
    SCAN_REQUESTS.inc()
    ACTIVE_SCANS.inc()
    
    try:
        response = await call_next(request)
        duration = time.time() - start_time
        SCAN_DURATION.observe(duration)
        return response
    finally:
        ACTIVE_SCANS.dec()
```

#### 系统层监控
```yaml
# docker-compose.yml 监控配置
version: '3.8'
services:
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

  node-exporter:
    image: prom/node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
```

### 3.2 自动扩缩容策略

#### Kubernetes HPA 配置
```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: website-scanner-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: website-scanner
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
```

## 四、性能测试方案

### 4.1 压力测试脚本
```python
# performance_test.py
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor

class PerformanceTester:
    def __init__(self, base_url: str, max_concurrent: int = 100):
        self.base_url = base_url
        self.max_concurrent = max_concurrent
        self.results = []
    
    async def test_scan_api(self, domain: str):
        """测试扫描API性能"""
        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            try:
                async with session.post(
                    f"{self.base_url}/api/v1/scan",
                    json={"domain": domain, "scan_type": "basic"},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    duration = time.time() - start_time
                    return {
                        "domain": domain,
                        "status": response.status,
                        "duration": duration,
                        "success": response.status == 200
                    }
            except Exception as e:
                duration = time.time() - start_time
                return {
                    "domain": domain,
                    "status": "error",
                    "duration": duration,
                    "success": False,
                    "error": str(e)
                }
    
    async def run_concurrent_test(self, domains: list):
        """并发测试"""
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def test_with_semaphore(domain):
            async with semaphore:
                return await self.test_scan_api(domain)
        
        tasks = [test_with_semaphore(domain) for domain in domains]
        results = await asyncio.gather(*tasks)
        
        # 统计结果
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        avg_duration = sum(r["duration"] for r in results) / len(results)
        
        return {
            "total": len(results),
            "successful": len(successful),
            "failed": len(failed),
            "success_rate": len(successful) / len(results) * 100,
            "avg_duration": avg_duration,
            "results": results
        }

# 使用示例
async def main():
    tester = PerformanceTester("http://localhost:8001")
    
    # 生成测试域名
    test_domains = [f"test{i}.example.com" for i in range(100)]
    
    # 运行并发测试
    result = await tester.run_concurrent_test(test_domains)
    
    print(f"测试结果:")
    print(f"总请求数: {result['total']}")
    print(f"成功数: {result['successful']}")
    print(f"失败数: {result['failed']}")
    print(f"成功率: {result['success_rate']:.2f}%")
    print(f"平均响应时间: {result['avg_duration']:.2f}秒")

if __name__ == "__main__":
    asyncio.run(main())
```

## 五、性能优化检查清单

### 5.1 数据库优化
- [ ] 创建合适的索引
- [ ] 配置连接池
- [ ] 启用查询缓存
- [ ] 定期清理过期数据
- [ ] 监控慢查询

### 5.2 缓存优化
- [ ] 实现多级缓存策略
- [ ] 配置缓存预热
- [ ] 设置合理的TTL
- [ ] 监控缓存命中率
- [ ] 实现缓存穿透保护

### 5.3 异步处理
- [ ] 使用异步数据库操作
- [ ] 配置任务队列
- [ ] 实现并发控制
- [ ] 监控队列长度
- [ ] 设置任务超时

### 5.4 负载均衡
- [ ] 配置健康检查
- [ ] 实现会话保持
- [ ] 设置限流规则
- [ ] 监控负载分布
- [ ] 配置故障转移

### 5.5 监控告警
- [ ] 部署监控系统
- [ ] 设置性能告警
- [ ] 配置日志聚合
- [ ] 实现自动扩缩容
- [ ] 定期性能评估

## 总结

通过以上性能优化方案，系统可以：

1. **处理大规模数据**：支持每日10,000+网站扫描
2. **高并发处理**：支持1000+并发连接
3. **快速响应**：API响应时间<200ms
4. **高可用性**：系统可用性>99.9%
5. **自动扩展**：根据负载自动扩缩容
6. **实时监控**：全面的性能监控和告警

这些优化措施确保了系统在大规模数据处理场景下的稳定性和性能。 