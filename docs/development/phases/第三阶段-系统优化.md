# 第三阶段：系统优化（3-5个月）

## 阶段目标

### 总体目标
在第二阶段基础上，进行全面的系统优化和性能提升，实现高可用、高性能的内容合规检测系统，建立完善的数据分析和可视化体系。

### 详细目标

#### 1. 性能优化目标
- **系统性能**：API响应时间 < 100ms（95%分位数）
- **并发能力**：支持1000+并发连接
- **吞吐量**：处理能力 > 5000 sites/min
- **可用性**：系统可用性 > 99.9%
- **内存优化**：内存使用率 < 70%，零内存泄漏
- **资源效率**：CPU使用率 < 80%，磁盘IO优化

#### 2. 企业级架构目标
- **高可用架构**：实现99.9%系统可用性，支持故障自动切换
- **分布式扩展**：支持多地域部署，单集群支持100+节点
- **微服务治理**：完善服务注册发现、配置中心、链路追踪
- **云原生部署**：支持Kubernetes编排，实现弹性伸缩

#### 3. 智能化升级目标
- **机器学习能力**：
  - 异常检测模型准确率≥90%
  - 威胁分类模型F1-Score≥0.85
  - 误报率降低至≤2%
  - 支持模型在线学习和更新
- **智能分析**：
  - 威胁情报自动关联分析
  - 攻击模式识别和预测
  - 安全态势感知和风险评估
  - 智能告警过滤和优先级排序
- **自适应优化**：
  - 检测策略自动调优
  - 资源分配动态优化
  - 告警阈值自适应调整

#### 4. 用户体验目标
- **移动端应用**：
  - 支持iOS/Android原生应用
  - 实时告警推送和处理
  - 移动端操作响应时间≤2秒
  - 离线数据同步功能
- **Web界面优化**：
  - 页面加载时间≤3秒
  - 支持多语言国际化
  - 响应式设计适配各种屏幕
  - 用户操作路径优化50%
- **API开放平台**：
  - 提供完整的RESTful API
  - API响应时间≤500ms
  - 支持第三方系统集成
  - API调用量支持10万次/天

#### 5. 运维自动化目标
- **监控告警体系**：
  - 建立全链路监控（APM）
  - 自定义监控指标≥50个
  - 告警规则智能化配置
  - 故障自动诊断和恢复
- **自动化运维**：
  - 系统自动扩缩容
  - 故障自动切换≤30秒
  - 数据自动备份和恢复
  - 性能自动调优
- **DevOps流程**：
  - CI/CD流水线自动化
  - 代码质量门禁
  - 自动化测试覆盖率≥90%
  - 灰度发布和回滚机制

#### 6. 安全合规目标
- **数据安全**：
  - 敏感数据加密存储
  - 数据传输TLS加密
  - 数据脱敏和匿名化
  - 数据备份加密
- **访问控制**：
  - 基于角色的权限控制（RBAC）
  - 多因子认证（MFA）
  - API访问令牌管理
  - 操作审计日志
- **合规认证**：
  - 等保三级认证准备
  - GDPR数据保护合规
  - 行业安全标准对接
  - 安全漏洞扫描和修复

#### 7. 业务价值目标
- **成本效益**：
  - 运维成本降低60%
  - 人力投入减少50%
  - 硬件资源利用率提升40%
  - 检测效率提升10倍
- **业务支撑**：
  - 支持多租户SaaS模式
  - 支持私有化部署
  - 支持混合云架构
  - 业务连续性保障99.9%
- **生态建设**：
  - 建立合作伙伴API生态
  - 威胁情报共享机制
  - 开源社区贡献
  - 技术标准制定参与

#### 8. 创新探索目标
- **前沿技术**：
  - 大模型在内容审核中的应用
  - 联邦学习在威胁检测中的探索
  - 边缘计算在流量分析中的应用
  - 区块链在审计溯源中的尝试
- **产品创新**：
  - 零信任安全架构探索
  - 自适应安全防护
  - 威胁狩猎平台
  - 安全编排自动化响应（SOAR）

## 开发顺序

1. **性能优化与分布式架构**（系统架构升级）
2. **智能分析与机器学习**（检测能力提升）
3. **移动端与API开放**（用户体验提升）
4. **运维监控与自动化**（系统可靠性提升）

## 详细模块设计

### 1. 性能优化与分布式架构

**优化目标：**
- 支持10万+网站并发检测
- 流量分析处理能力达到10Gbps
- 系统可用性达到99.9%
- 响应时间优化到秒级

**分布式架构设计：**
```yaml
# docker-compose-cluster.yml
version: '3.8'

services:
  # 负载均衡器
  nginx-lb:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - web-backend-1
      - web-backend-2
      - web-backend-3

  # Web后端集群
  web-backend-1:
    build: ./web-backend
    environment:
      - INSTANCE_ID=web-1
      - REDIS_URL=redis://redis-cluster:6379
      - MONGODB_URL=mongodb://mongo-1:27017,mongo-2:27017,mongo-3:27017/compliance?replicaSet=rs0
    depends_on:
      - redis-cluster
      - mongo-1

  web-backend-2:
    build: ./web-backend
    environment:
      - INSTANCE_ID=web-2
      - REDIS_URL=redis://redis-cluster:6379
      - MONGODB_URL=mongodb://mongo-1:27017,mongo-2:27017,mongo-3:27017/compliance?replicaSet=rs0
    depends_on:
      - redis-cluster
      - mongo-1

  web-backend-3:
    build: ./web-backend
    environment:
      - INSTANCE_ID=web-3
      - REDIS_URL=redis://redis-cluster:6379
      - MONGODB_URL=mongodb://mongo-1:27017,mongo-2:27017,mongo-3:27017/compliance?replicaSet=rs0
    depends_on:
      - redis-cluster
      - mongo-1

  # 网站检测集群
  website-detector-1:
    build: ./website-detector
    environment:
      - WORKER_ID=detector-1
      - CELERY_BROKER=redis://redis-cluster:6379/1
      - MONGODB_URL=mongodb://mongo-1:27017,mongo-2:27017,mongo-3:27017/compliance?replicaSet=rs0
    depends_on:
      - redis-cluster
      - mongo-1

  website-detector-2:
    build: ./website-detector
    environment:
      - WORKER_ID=detector-2
      - CELERY_BROKER=redis://redis-cluster:6379/1
      - MONGODB_URL=mongodb://mongo-1:27017,mongo-2:27017,mongo-3:27017/compliance?replicaSet=rs0
    depends_on:
      - redis-cluster
      - mongo-1

  website-detector-3:
    build: ./website-detector
    environment:
      - WORKER_ID=detector-3
      - CELERY_BROKER=redis://redis-cluster:6379/1
      - MONGODB_URL=mongodb://mongo-1:27017,mongo-2:27017,mongo-3:27017/compliance?replicaSet=rs0
    depends_on:
      - redis-cluster
      - mongo-1

  # 流量分析集群
  traffic-analyzer-1:
    build: ./traffic-analyzer
    environment:
      - ANALYZER_ID=traffic-1
      - INTERFACE=eth0
      - ELASTICSEARCH_URL=http://es-1:9200,http://es-2:9200,http://es-3:9200
    network_mode: host
    privileged: true
    depends_on:
      - es-1

  traffic-analyzer-2:
    build: ./traffic-analyzer
    environment:
      - ANALYZER_ID=traffic-2
      - INTERFACE=eth1
      - ELASTICSEARCH_URL=http://es-1:9200,http://es-2:9200,http://es-3:9200
    network_mode: host
    privileged: true
    depends_on:
      - es-1

  # MongoDB集群
  mongo-1:
    image: mongo:5.0
    command: mongod --replSet rs0 --bind_ip_all
    volumes:
      - mongo1-data:/data/db
    ports:
      - "27017:27017"

  mongo-2:
    image: mongo:5.0
    command: mongod --replSet rs0 --bind_ip_all
    volumes:
      - mongo2-data:/data/db
    ports:
      - "27018:27017"

  mongo-3:
    image: mongo:5.0
    command: mongod --replSet rs0 --bind_ip_all
    volumes:
      - mongo3-data:/data/db
    ports:
      - "27019:27017"

  # Redis集群
  redis-cluster:
    image: redis:7-alpine
    command: redis-server --appendonly yes --cluster-enabled yes
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"

  # Elasticsearch集群
  es-1:
    image: elasticsearch:8.8.0
    environment:
      - node.name=es-1
      - cluster.name=compliance-cluster
      - discovery.seed_hosts=es-2,es-3
      - cluster.initial_master_nodes=es-1,es-2,es-3
      - xpack.security.enabled=false
    volumes:
      - es1-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  es-2:
    image: elasticsearch:8.8.0
    environment:
      - node.name=es-2
      - cluster.name=compliance-cluster
      - discovery.seed_hosts=es-1,es-3
      - cluster.initial_master_nodes=es-1,es-2,es-3
      - xpack.security.enabled=false
    volumes:
      - es2-data:/usr/share/elasticsearch/data
    ports:
      - "9201:9200"

  es-3:
    image: elasticsearch:8.8.0
    environment:
      - node.name=es-3
      - cluster.name=compliance-cluster
      - discovery.seed_hosts=es-1,es-2
      - cluster.initial_master_nodes=es-1,es-2,es-3
      - xpack.security.enabled=false
    volumes:
      - es3-data:/usr/share/elasticsearch/data
    ports:
      - "9202:9200"

  # 监控组件
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources

  # 日志收集
  filebeat:
    image: elastic/filebeat:8.8.0
    volumes:
      - ./monitoring/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - es-1

volumes:
  mongo1-data:
  mongo2-data:
  mongo3-data:
  redis-data:
  es1-data:
  es2-data:
  es3-data:
  prometheus-data:
  grafana-data:
```

**性能优化实现：**
```python
# performance_optimizer.py
import asyncio
import aioredis
import motor.motor_asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import time
import hashlib

@dataclass
class CacheConfig:
    redis_url: str
    default_ttl: int = 3600
    max_connections: int = 100

@dataclass
class DatabaseConfig:
    mongodb_url: str
    max_pool_size: int = 100
    min_pool_size: int = 10

class PerformanceOptimizer:
    def __init__(self, cache_config: CacheConfig, db_config: DatabaseConfig):
        self.cache_config = cache_config
        self.db_config = db_config
        self.redis_pool = None
        self.mongo_client = None
        self.thread_pool = ThreadPoolExecutor(max_workers=50)
    
    async def initialize(self):
        """初始化连接池"""
        # Redis连接池
        self.redis_pool = aioredis.ConnectionPool.from_url(
            self.cache_config.redis_url,
            max_connections=self.cache_config.max_connections
        )
        
        # MongoDB连接
        self.mongo_client = motor.motor_asyncio.AsyncIOMotorClient(
            self.db_config.mongodb_url,
            maxPoolSize=self.db_config.max_pool_size,
            minPoolSize=self.db_config.min_pool_size
        )
    
    async def get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """获取缓存结果"""
        try:
            redis = aioredis.Redis(connection_pool=self.redis_pool)
            cached_data = await redis.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            print(f"缓存读取失败: {e}")
        return None
    
    async def set_cached_result(self, cache_key: str, data: Dict, ttl: int = None):
        """设置缓存结果"""
        try:
            redis = aioredis.Redis(connection_pool=self.redis_pool)
            ttl = ttl or self.cache_config.default_ttl
            await redis.setex(cache_key, ttl, json.dumps(data))
        except Exception as e:
            print(f"缓存写入失败: {e}")
    
    def generate_cache_key(self, prefix: str, params: Dict) -> str:
        """生成缓存键"""
        # 将参数排序后生成哈希
        sorted_params = sorted(params.items())
        param_str = '&'.join([f"{k}={v}" for k, v in sorted_params])
        hash_str = hashlib.md5(param_str.encode()).hexdigest()
        return f"{prefix}:{hash_str}"
    
    async def batch_website_detection(self, urls: List[str]) -> List[Dict]:
        """批量网站检测（优化版）"""
        results = []
        
        # 分批处理，每批20个URL
        batch_size = 20
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            
            # 并发检测
            tasks = []
            for url in batch_urls:
                cache_key = self.generate_cache_key('website_detection', {'url': url})
                
                # 先检查缓存
                cached_result = await self.get_cached_result(cache_key)
                if cached_result:
                    results.append(cached_result)
                else:
                    # 创建检测任务
                    task = asyncio.create_task(self._detect_website_with_cache(url, cache_key))
                    tasks.append(task)
            
            # 等待批次完成
            if tasks:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                for result in batch_results:
                    if not isinstance(result, Exception):
                        results.append(result)
            
            # 批次间隔，避免过载
            await asyncio.sleep(0.1)
        
        return results
    
    async def _detect_website_with_cache(self, url: str, cache_key: str) -> Dict:
        """带缓存的网站检测"""
        try:
            # 执行检测（这里调用实际的检测逻辑）
            result = await self._perform_website_detection(url)
            
            # 缓存结果（成功结果缓存1小时，失败结果缓存10分钟）
            ttl = 3600 if result.get('status') == 'success' else 600
            await self.set_cached_result(cache_key, result, ttl)
            
            return result
        except Exception as e:
            error_result = {
                'url': url,
                'status': 'error',
                'error': str(e),
                'timestamp': time.time()
            }
            await self.set_cached_result(cache_key, error_result, 600)
            return error_result
    
    async def _perform_website_detection(self, url: str) -> Dict:
        """执行实际的网站检测"""
        # 这里是实际的检测逻辑，可以调用之前实现的检测服务
        # 为了示例，这里返回模拟结果
        await asyncio.sleep(0.5)  # 模拟检测耗时
        
        return {
            'url': url,
            'status': 'success',
            'compliance_score': 85,
            'issues': [],
            'timestamp': time.time()
        }
    
    async def optimized_traffic_analysis(self, traffic_data: List[Dict]) -> Dict:
        """优化的流量分析"""
        # 使用多进程处理大量流量数据
        chunk_size = 1000
        chunks = [traffic_data[i:i + chunk_size] for i in range(0, len(traffic_data), chunk_size)]
        
        # 并行处理各个数据块
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(self.thread_pool, self._analyze_traffic_chunk, chunk)
            for chunk in chunks
        ]
        
        chunk_results = await asyncio.gather(*tasks)
        
        # 合并结果
        total_alerts = 0
        alert_types = {}
        risk_distribution = {'high': 0, 'medium': 0, 'low': 0}
        
        for chunk_result in chunk_results:
            total_alerts += chunk_result['alerts']
            for alert_type, count in chunk_result['alert_types'].items():
                alert_types[alert_type] = alert_types.get(alert_type, 0) + count
            for risk_level, count in chunk_result['risk_distribution'].items():
                risk_distribution[risk_level] += count
        
        return {
            'total_alerts': total_alerts,
            'alert_types': alert_types,
            'risk_distribution': risk_distribution,
            'processing_time': time.time()
        }
    
    def _analyze_traffic_chunk(self, traffic_chunk: List[Dict]) -> Dict:
        """分析流量数据块（CPU密集型任务）"""
        alerts = 0
        alert_types = {}
        risk_distribution = {'high': 0, 'medium': 0, 'low': 0}
        
        for traffic_record in traffic_chunk:
            # 模拟流量分析逻辑
            if self._is_suspicious_traffic(traffic_record):
                alerts += 1
                alert_type = self._classify_alert_type(traffic_record)
                alert_types[alert_type] = alert_types.get(alert_type, 0) + 1
                
                risk_level = self._calculate_risk_level(traffic_record)
                risk_distribution[risk_level] += 1
        
        return {
            'alerts': alerts,
            'alert_types': alert_types,
            'risk_distribution': risk_distribution
        }
    
    def _is_suspicious_traffic(self, traffic_record: Dict) -> bool:
        """判断是否为可疑流量"""
        # 简化的判断逻辑
        return traffic_record.get('packet_count', 0) > 1000
    
    def _classify_alert_type(self, traffic_record: Dict) -> str:
        """分类告警类型"""
        # 简化的分类逻辑
        port = traffic_record.get('dest_port', 0)
        if port == 80 or port == 443:
            return 'web_attack'
        elif port == 22:
            return 'ssh_attack'
        else:
            return 'other'
    
    def _calculate_risk_level(self, traffic_record: Dict) -> str:
        """计算风险等级"""
        packet_count = traffic_record.get('packet_count', 0)
        if packet_count > 10000:
            return 'high'
        elif packet_count > 1000:
            return 'medium'
        else:
            return 'low'
    
    async def cleanup(self):
        """清理资源"""
        if self.redis_pool:
            await self.redis_pool.disconnect()
        if self.mongo_client:
            self.mongo_client.close()
        self.thread_pool.shutdown(wait=True)
```

### 2. 智能分析与机器学习

**机器学习模块设计：**
```python
# ml_analyzer.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
from typing import Dict, List, Tuple
import logging

class MLAnalyzer:
    def __init__(self, model_path: str = './models/'):
        self.model_path = model_path
        self.anomaly_detector = None
        self.threat_classifier = None
        self.scaler = StandardScaler()
        self.logger = logging.getLogger(__name__)
    
    def train_anomaly_detector(self, normal_traffic_data: List[Dict]):
        """训练异常检测模型"""
        # 特征提取
        features = self._extract_traffic_features(normal_traffic_data)
        
        # 数据预处理
        X = self.scaler.fit_transform(features)
        
        # 训练Isolation Forest模型
        self.anomaly_detector = IsolationForest(
            contamination=0.1,  # 假设10%的数据是异常
            random_state=42,
            n_estimators=100
        )
        self.anomaly_detector.fit(X)
        
        # 保存模型
        joblib.dump(self.anomaly_detector, f'{self.model_path}/anomaly_detector.pkl')
        joblib.dump(self.scaler, f'{self.model_path}/scaler.pkl')
        
        self.logger.info("异常检测模型训练完成")
    
    def train_threat_classifier(self, labeled_data: List[Dict]):
        """训练威胁分类模型"""
        # 特征提取和标签准备
        features = []
        labels = []
        
        for record in labeled_data:
            feature_vector = self._extract_threat_features(record)
            features.append(feature_vector)
            labels.append(record['threat_type'])
        
        X = np.array(features)
        y = np.array(labels)
        
        # 数据分割
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # 训练随机森林分类器
        self.threat_classifier = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            random_state=42,
            class_weight='balanced'
        )
        self.threat_classifier.fit(X_train, y_train)
        
        # 评估模型
        y_pred = self.threat_classifier.predict(X_test)
        report = classification_report(y_test, y_pred)
        self.logger.info(f"威胁分类模型评估结果:\n{report}")
        
        # 保存模型
        joblib.dump(self.threat_classifier, f'{self.model_path}/threat_classifier.pkl')
        
        self.logger.info("威胁分类模型训练完成")
    
    def load_models(self):
        """加载训练好的模型"""
        try:
            self.anomaly_detector = joblib.load(f'{self.model_path}/anomaly_detector.pkl')
            self.threat_classifier = joblib.load(f'{self.model_path}/threat_classifier.pkl')
            self.scaler = joblib.load(f'{self.model_path}/scaler.pkl')
            self.logger.info("模型加载成功")
        except FileNotFoundError as e:
            self.logger.error(f"模型文件不存在: {e}")
    
    def detect_anomaly(self, traffic_record: Dict) -> Dict:
        """检测流量异常"""
        if not self.anomaly_detector:
            return {'is_anomaly': False, 'confidence': 0.0, 'error': '模型未加载'}
        
        # 特征提取
        features = self._extract_traffic_features([traffic_record])
        X = self.scaler.transform(features)
        
        # 异常检测
        anomaly_score = self.anomaly_detector.decision_function(X)[0]
        is_anomaly = self.anomaly_detector.predict(X)[0] == -1
        
        # 计算置信度（将异常分数转换为0-1之间的置信度）
        confidence = max(0, min(1, (0.5 - anomaly_score) * 2))
        
        return {
            'is_anomaly': is_anomaly,
            'confidence': confidence,
            'anomaly_score': anomaly_score
        }
    
    def classify_threat(self, traffic_record: Dict) -> Dict:
        """分类威胁类型"""
        if not self.threat_classifier:
            return {'threat_type': 'unknown', 'confidence': 0.0, 'error': '模型未加载'}
        
        # 特征提取
        features = self._extract_threat_features(traffic_record)
        X = np.array([features])
        
        # 威胁分类
        threat_type = self.threat_classifier.predict(X)[0]
        probabilities = self.threat_classifier.predict_proba(X)[0]
        confidence = max(probabilities)
        
        # 获取所有类别的概率
        classes = self.threat_classifier.classes_
        threat_probabilities = dict(zip(classes, probabilities))
        
        return {
            'threat_type': threat_type,
            'confidence': confidence,
            'probabilities': threat_probabilities
        }
    
    def _extract_traffic_features(self, traffic_data: List[Dict]) -> np.ndarray:
        """提取流量特征"""
        features = []
        
        for record in traffic_data:
            feature_vector = [
                record.get('packet_count', 0),
                record.get('byte_count', 0),
                record.get('duration', 0),
                record.get('src_port', 0),
                record.get('dest_port', 0),
                record.get('protocol', 0),  # TCP=6, UDP=17, etc.
                record.get('flags', 0),
                record.get('packets_per_second', 0),
                record.get('bytes_per_packet', 0),
                record.get('unique_src_ips', 0),
                record.get('unique_dest_ips', 0),
                record.get('connection_count', 0)
            ]
            features.append(feature_vector)
        
        return np.array(features)
    
    def _extract_threat_features(self, record: Dict) -> List[float]:
        """提取威胁分类特征"""
        return [
            record.get('packet_count', 0),
            record.get('byte_count', 0),
            record.get('duration', 0),
            record.get('src_port', 0),
            record.get('dest_port', 0),
            record.get('protocol', 0),
            record.get('flags', 0),
            record.get('packets_per_second', 0),
            record.get('bytes_per_packet', 0),
            record.get('syn_count', 0),
            record.get('ack_count', 0),
            record.get('fin_count', 0),
            record.get('rst_count', 0),
            record.get('payload_entropy', 0),  # 载荷熵值
            record.get('header_length', 0),
            record.get('window_size', 0),
            record.get('ttl', 0),
            record.get('fragment_count', 0),
            record.get('retransmission_count', 0),
            record.get('out_of_order_count', 0)
        ]
    
    def intelligent_alert_filtering(self, alerts: List[Dict]) -> List[Dict]:
        """智能告警过滤"""
        filtered_alerts = []
        
        for alert in alerts:
            # 使用机器学习模型评估告警
            anomaly_result = self.detect_anomaly(alert)
            threat_result = self.classify_threat(alert)
            
            # 计算综合风险分数
            risk_score = self._calculate_risk_score(alert, anomaly_result, threat_result)
            
            # 只保留高风险告警
            if risk_score > 0.7:
                alert['ml_risk_score'] = risk_score
                alert['ml_threat_type'] = threat_result['threat_type']
                alert['ml_confidence'] = threat_result['confidence']
                filtered_alerts.append(alert)
        
        return filtered_alerts
    
    def _calculate_risk_score(self, alert: Dict, anomaly_result: Dict, threat_result: Dict) -> float:
        """计算综合风险分数"""
        # 基础风险分数
        base_score = 0.3
        
        # 异常检测分数权重
        anomaly_weight = 0.4
        anomaly_score = anomaly_result['confidence'] if anomaly_result['is_anomaly'] else 0
        
        # 威胁分类分数权重
        threat_weight = 0.3
        threat_score = threat_result['confidence']
        
        # 威胁类型加权
        threat_type_weights = {
            'ddos': 1.0,
            'malware': 0.9,
            'intrusion': 0.8,
            'scan': 0.6,
            'spam': 0.4,
            'normal': 0.1
        }
        threat_type = threat_result['threat_type']
        threat_type_weight = threat_type_weights.get(threat_type, 0.5)
        
        # 计算最终风险分数
        final_score = (
            base_score +
            anomaly_weight * anomaly_score +
            threat_weight * threat_score * threat_type_weight
        )
        
        return min(1.0, final_score)
    
    def generate_threat_intelligence(self, alerts: List[Dict]) -> Dict:
        """生成威胁情报"""
        if not alerts:
            return {'threat_trends': {}, 'attack_patterns': {}, 'recommendations': []}
        
        # 威胁趋势分析
        threat_trends = self._analyze_threat_trends(alerts)
        
        # 攻击模式识别
        attack_patterns = self._identify_attack_patterns(alerts)
        
        # 生成安全建议
        recommendations = self._generate_recommendations(threat_trends, attack_patterns)
        
        return {
            'threat_trends': threat_trends,
            'attack_patterns': attack_patterns,
            'recommendations': recommendations,
            'analysis_time': pd.Timestamp.now().isoformat()
        }
    
    def _analyze_threat_trends(self, alerts: List[Dict]) -> Dict:
        """分析威胁趋势"""
        df = pd.DataFrame(alerts)
        
        # 按时间分组统计
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['hour'] = df['timestamp'].dt.hour
        
        hourly_trends = df.groupby(['hour', 'ml_threat_type']).size().unstack(fill_value=0)
        
        return {
            'hourly_distribution': hourly_trends.to_dict(),
            'peak_hours': hourly_trends.sum(axis=1).nlargest(3).index.tolist(),
            'dominant_threats': df['ml_threat_type'].value_counts().head(5).to_dict()
        }
    
    def _identify_attack_patterns(self, alerts: List[Dict]) -> Dict:
        """识别攻击模式"""
        df = pd.DataFrame(alerts)
        
        # 识别协同攻击（同一源IP的多种攻击类型）
        coordinated_attacks = df.groupby('src_ip')['ml_threat_type'].nunique()
        coordinated_ips = coordinated_attacks[coordinated_attacks > 1].index.tolist()
        
        # 识别分布式攻击（多个源IP攻击同一目标）
        distributed_attacks = df.groupby('dest_ip')['src_ip'].nunique()
        distributed_targets = distributed_attacks[distributed_attacks > 10].index.tolist()
        
        # 识别时间模式（攻击时间集中度）
        time_patterns = df.groupby(df['timestamp'].dt.hour).size()
        concentrated_hours = time_patterns[time_patterns > time_patterns.mean() + 2 * time_patterns.std()].index.tolist()
        
        return {
            'coordinated_attack_ips': coordinated_ips[:10],
            'distributed_attack_targets': distributed_targets[:10],
            'concentrated_attack_hours': concentrated_hours,
            'attack_frequency': len(alerts) / 24  # 每小时平均攻击次数
        }
    
    def _generate_recommendations(self, threat_trends: Dict, attack_patterns: Dict) -> List[str]:
        """生成安全建议"""
        recommendations = []
        
        # 基于威胁趋势的建议
        dominant_threats = threat_trends.get('dominant_threats', {})
        if 'ddos' in dominant_threats and dominant_threats['ddos'] > 100:
            recommendations.append("检测到大量DDoS攻击，建议启用DDoS防护服务")
        
        if 'scan' in dominant_threats and dominant_threats['scan'] > 50:
            recommendations.append("检测到频繁端口扫描，建议加强防火墙规则")
        
        # 基于攻击模式的建议
        if len(attack_patterns.get('coordinated_attack_ips', [])) > 5:
            recommendations.append("检测到协同攻击，建议对可疑IP进行深度分析")
        
        if len(attack_patterns.get('distributed_attack_targets', [])) > 3:
            recommendations.append("检测到分布式攻击，建议对重要资产加强保护")
        
        if attack_patterns.get('attack_frequency', 0) > 10:
            recommendations.append("攻击频率较高，建议提升监控告警级别")
        
        # 基于时间模式的建议
        concentrated_hours = attack_patterns.get('concentrated_attack_hours', [])
        if concentrated_hours:
            recommendations.append(f"攻击集中在{concentrated_hours}时段，建议在这些时间加强监控")
        
        return recommendations
```

### 3. 移动端与API开放

**移动端应用架构：**
```typescript
// mobile-app/src/types/api.ts
export interface AlertSummary {
  total_alerts: number;
  high_risk_alerts: number;
  new_alerts_24h: number;
  blocked_ips: number;
}

export interface RealtimeAlert {
  id: string;
  timestamp: string;
  src_ip: string;
  dest_ip: string;
  alert_type: string;
  risk_level: 'high' | 'medium' | 'low';
  signature: string;
  status: 'new' | 'processing' | 'resolved';
}

export interface SystemStatus {
  website_detector: 'online' | 'offline' | 'warning';
  traffic_analyzer: 'online' | 'offline' | 'warning';
  alert_processor: 'online' | 'offline' | 'warning';
  database: 'online' | 'offline' | 'warning';
}

// mobile-app/src/services/api.ts
import axios, { AxiosInstance } from 'axios';

class ComplianceAPI {
  private client: AxiosInstance;
  
  constructor(baseURL: string, apiKey: string) {
    this.client = axios.create({
      baseURL,
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      timeout: 10000
    });
    
    // 请求拦截器
    this.client.interceptors.request.use(
      (config) => {
        console.log(`API Request: ${config.method?.toUpperCase()} ${config.url}`);
        return config;
      },
      (error) => Promise.reject(error)
    );
    
    // 响应拦截器
    this.client.interceptors.response.use(
      (response) => response,
      (error) => {
        console.error('API Error:', error.response?.data || error.message);
        return Promise.reject(error);
      }
    );
  }
  
  // 获取告警摘要
  async getAlertSummary(): Promise<AlertSummary> {
    const response = await this.client.get('/api/alerts/summary');
    return response.data;
  }
  
  // 获取实时告警
  async getRealtimeAlerts(limit: number = 20): Promise<RealtimeAlert[]> {
    const response = await this.client.get(`/api/alerts/realtime?limit=${limit}`);
    return response.data.alerts;
  }
  
  // 获取系统状态
  async getSystemStatus(): Promise<SystemStatus> {
    const response = await this.client.get('/api/system/status');
    return response.data;
  }
  
  // 处理告警
  async handleAlert(alertId: string, action: string): Promise<void> {
    await this.client.post(`/api/alerts/${alertId}/handle`, { action });
  }
  
  // 封禁IP
  async blockIP(ip: string, duration: number = 3600): Promise<void> {
    await this.client.post('/api/security/block-ip', { ip, duration });
  }
  
  // 获取网站检测结果
  async getWebsiteDetectionResults(page: number = 1, limit: number = 20) {
    const response = await this.client.get(`/api/website-detection/results?page=${page}&limit=${limit}`);
    return response.data;
  }
  
  // 启动网站检测
  async startWebsiteDetection(urls: string[]): Promise<string> {
    const response = await this.client.post('/api/website-detection/start', { urls });
    return response.data.task_id;
  }
  
  // 获取检测任务状态
  async getDetectionTaskStatus(taskId: string) {
    const response = await this.client.get(`/api/tasks/${taskId}/status`);
    return response.data;
  }
}

export default ComplianceAPI;
```

**React Native移动端主界面：**
```tsx
// mobile-app/src/screens/DashboardScreen.tsx
import React, { useState, useEffect } from 'react';
import {
  View,
  Text,
  StyleSheet,
  ScrollView,
  RefreshControl,
  Alert,
  TouchableOpacity
} from 'react-native';
import { Card, Button, Badge, List } from 'react-native-elements';
import Icon from 'react-native-vector-icons/MaterialIcons';
import ComplianceAPI from '../services/api';
import { AlertSummary, RealtimeAlert, SystemStatus } from '../types/api';

const DashboardScreen: React.FC = () => {
  const [alertSummary, setAlertSummary] = useState<AlertSummary | null>(null);
  const [realtimeAlerts, setRealtimeAlerts] = useState<RealtimeAlert[]>([]);
  const [systemStatus, setSystemStatus] = useState<SystemStatus | null>(null);
  const [refreshing, setRefreshing] = useState(false);
  const [api] = useState(new ComplianceAPI('https://api.compliance.com', 'your-api-key'));
  
  useEffect(() => {
    loadDashboardData();
    
    // 设置定时刷新
    const interval = setInterval(loadDashboardData, 30000); // 30秒刷新一次
    return () => clearInterval(interval);
  }, []);
  
  const loadDashboardData = async () => {
    try {
      const [summary, alerts, status] = await Promise.all([
        api.getAlertSummary(),
        api.getRealtimeAlerts(10),
        api.getSystemStatus()
      ]);
      
      setAlertSummary(summary);
      setRealtimeAlerts(alerts);
      setSystemStatus(status);
    } catch (error) {
      Alert.alert('错误', '加载数据失败，请检查网络连接');
    }
  };
  
  const onRefresh = async () => {
    setRefreshing(true);
    await loadDashboardData();
    setRefreshing(false);
  };
  
  const handleAlert = async (alert: RealtimeAlert) => {
    Alert.alert(
      '处理告警',
      `是否要处理告警: ${alert.signature}?`,
      [
        { text: '取消', style: 'cancel' },
        {
          text: '标记已处理',
          onPress: async () => {
            try {
              await api.handleAlert(alert.id, 'resolve');
              await loadDashboardData();
              Alert.alert('成功', '告警已标记为已处理');
            } catch (error) {
              Alert.alert('错误', '处理失败');
            }
          }
        },
        {
          text: '封禁IP',
          style: 'destructive',
          onPress: async () => {
            try {
              await api.blockIP(alert.src_ip);
              await loadDashboardData();
              Alert.alert('成功', `IP ${alert.src_ip} 已被封禁`);
            } catch (error) {
              Alert.alert('错误', '封禁失败');
            }
          }
        }
      ]
    );
  };
  
  const getStatusColor = (status: string) => {
    switch (status) {
      case 'online': return '#4CAF50';
      case 'warning': return '#FF9800';
      case 'offline': return '#F44336';
      default: return '#9E9E9E';
    }
  };
  
  const getRiskLevelColor = (level: string) => {
    switch (level) {
      case 'high': return 'error';
      case 'medium': return 'warning';
      case 'low': return 'success';
      default: return 'primary';
    }
  };
  
  return (
    <ScrollView
      style={styles.container}
      refreshControl={
        <RefreshControl refreshing={refreshing} onRefresh={onRefresh} />
      }
    >
      {/* 告警摘要卡片 */}
      {alertSummary && (
        <Card containerStyle={styles.card}>
          <Text style={styles.cardTitle}>告警摘要</Text>
          <View style={styles.summaryGrid}>
            <View style={styles.summaryItem}>
              <Text style={styles.summaryNumber}>{alertSummary.total_alerts}</Text>
              <Text style={styles.summaryLabel}>总告警数</Text>
            </View>
            <View style={styles.summaryItem}>
              <Text style={[styles.summaryNumber, { color: '#F44336' }]}>
                {alertSummary.high_risk_alerts}
              </Text>
              <Text style={styles.summaryLabel}>高风险</Text>
            </View>
            <View style={styles.summaryItem}>
              <Text style={[styles.summaryNumber, { color: '#FF9800' }]}>
                {alertSummary.new_alerts_24h}
              </Text>
              <Text style={styles.summaryLabel}>24h新增</Text>
            </View>
            <View style={styles.summaryItem}>
              <Text style={[styles.summaryNumber, { color: '#4CAF50' }]}>
                {alertSummary.blocked_ips}
              </Text>
              <Text style={styles.summaryLabel}>已封禁IP</Text>
            </View>
          </View>
        </Card>
      )}
      
      {/* 系统状态卡片 */}
      {systemStatus && (
        <Card containerStyle={styles.card}>
          <Text style={styles.cardTitle}>系统状态</Text>
          <View style={styles.statusGrid}>
            <View style={styles.statusItem}>
              <Icon
                name="language"
                size={24}
                color={getStatusColor(systemStatus.website_detector)}
              />
              <Text style={styles.statusLabel}>网站检测</Text>
            </View>
            <View style={styles.statusItem}>
              <Icon
                name="traffic"
                size={24}
                color={getStatusColor(systemStatus.traffic_analyzer)}
              />
              <Text style={styles.statusLabel}>流量分析</Text>
            </View>
            <View style={styles.statusItem}>
              <Icon
                name="warning"
                size={24}
                color={getStatusColor(systemStatus.alert_processor)}
              />
              <Text style={styles.statusLabel}>告警处理</Text>
            </View>
            <View style={styles.statusItem}>
              <Icon
                name="storage"
                size={24}
                color={getStatusColor(systemStatus.database)}
              />
              <Text style={styles.statusLabel}>数据库</Text>
            </View>
          </View>
        </Card>
      )}
      
      {/* 实时告警列表 */}
      <Card containerStyle={styles.card}>
        <View style={styles.cardHeader}>
          <Text style={styles.cardTitle}>实时告警</Text>
          <TouchableOpacity onPress={loadDashboardData}>
            <Icon name="refresh" size={24} color="#2196F3" />
          </TouchableOpacity>
        </View>
        
        {realtimeAlerts.length > 0 ? (
          <List containerStyle={styles.alertList}>
            {realtimeAlerts.map((alert) => (
              <TouchableOpacity
                key={alert.id}
                style={styles.alertItem}
                onPress={() => handleAlert(alert)}
              >
                <View style={styles.alertHeader}>
                  <Badge
                    value={alert.risk_level}
                    status={getRiskLevelColor(alert.risk_level)}
                    containerStyle={styles.alertBadge}
                  />
                  <Text style={styles.alertTime}>
                    {new Date(alert.timestamp).toLocaleTimeString()}
                  </Text>
                </View>
                <Text style={styles.alertSignature} numberOfLines={2}>
                  {alert.signature}
                </Text>
                <View style={styles.alertDetails}>
                  <Text style={styles.alertIP}>源IP: {alert.src_ip}</Text>
                  <Text style={styles.alertType}>{alert.alert_type}</Text>
                </View>
              </TouchableOpacity>
            ))}
          </List>
        ) : (
          <View style={styles.emptyState}>
            <Icon name="check-circle" size={48} color="#4CAF50" />
            <Text style={styles.emptyText}>暂无告警</Text>
          </View>
        )}
      </Card>
    </ScrollView>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#f5f5f5',
    padding: 16
  },
  card: {
    borderRadius: 8,
    marginBottom: 16,
    elevation: 2
  },
  cardTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    marginBottom: 16,
    color: '#333'
  },
  cardHeader: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    marginBottom: 16
  },
  summaryGrid: {
    flexDirection: 'row',
    justifyContent: 'space-between'
  },
  summaryItem: {
    alignItems: 'center',
    flex: 1
  },
  summaryNumber: {
    fontSize: 24,
    fontWeight: 'bold',
    color: '#2196F3'
  },
  summaryLabel: {
    fontSize: 12,
    color: '#666',
    marginTop: 4
  },
  statusGrid: {
    flexDirection: 'row',
    justifyContent: 'space-between'
  },
  statusItem: {
    alignItems: 'center',
    flex: 1
  },
  statusLabel: {
    fontSize: 12,
    color: '#666',
    marginTop: 4
  },
  alertList: {
    marginTop: 0
  },
  alertItem: {
    padding: 12,
    borderBottomWidth: 1,
    borderBottomColor: '#eee'
  },
  alertHeader: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    marginBottom: 8
  },
  alertBadge: {
    marginRight: 8
  },
  alertTime: {
    fontSize: 12,
    color: '#666'
  },
  alertSignature: {
    fontSize: 14,
    color: '#333',
    marginBottom: 8
  },
  alertDetails: {
    flexDirection: 'row',
    justifyContent: 'space-between'
  },
  alertIP: {
    fontSize: 12,
    color: '#666'
  },
  alertType: {
    fontSize: 12,
    color: '#2196F3',
    fontWeight: 'bold'
  },
  emptyState: {
    alignItems: 'center',
    padding: 32
  },
  emptyText: {
    fontSize: 16,
    color: '#666',
    marginTop: 8
  }
});

export default DashboardScreen;
```

### 4. 运维监控与自动化

**Prometheus监控配置：**
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # 监控Prometheus自身
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  # 监控Web后端服务
  - job_name: 'web-backend'
    static_configs:
      - targets: ['web-backend-1:8000', 'web-backend-2:8000', 'web-backend-3:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
  
  # 监控网站检测服务
  - job_name: 'website-detector'
    static_configs:
      - targets: ['website-detector-1:8001', 'website-detector-2:8001', 'website-detector-3:8001']
    metrics_path: '/metrics'
    scrape_interval: 30s
  
  # 监控流量分析服务
  - job_name: 'traffic-analyzer'
    static_configs:
      - targets: ['traffic-analyzer-1:8002', 'traffic-analyzer-2:8002']
    metrics_path: '/metrics'
    scrape_interval: 10s
  
  # 监控数据库
  - job_name: 'mongodb'
    static_configs:
      - targets: ['mongo-1:9216', 'mongo-2:9216', 'mongo-3:9216']
  
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-cluster:9121']
  
  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['es-1:9114', 'es-2:9114', 'es-3:9114']
  
  # 监控系统资源
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
```

**告警规则配置：**
```yaml
# monitoring/alert_rules.yml
groups:
  - name: compliance_system_alerts
    rules:
      # 服务可用性告警
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "服务 {{ $labels.job }} 不可用"
          description: "服务 {{ $labels.job }} 在实例 {{ $labels.instance }} 上已经停止运行超过1分钟"
      
      # 高CPU使用率告警
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "高CPU使用率"
          description: "实例 {{ $labels.instance }} CPU使用率超过80%，当前值: {{ $value }}%"
      
      # 高内存使用率告警
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "高内存使用率"
          description: "实例 {{ $labels.instance }} 内存使用率超过85%，当前值: {{ $value }}%"
      
      # 磁盘空间不足告警
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "磁盘空间不足"
          description: "实例 {{ $labels.instance }} 磁盘使用率超过90%，当前值: {{ $value }}%"
      
      # 网站检测任务积压告警
      - alert: WebsiteDetectionBacklog
        expr: website_detection_queue_size > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "网站检测任务积压"
          description: "网站检测队列积压任务数: {{ $value }}"
      
      # 流量分析延迟告警
      - alert: TrafficAnalysisDelay
        expr: traffic_analysis_processing_delay_seconds > 30
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "流量分析延迟"
          description: "流量分析处理延迟: {{ $value }}秒"
      
      # 高风险告警数量激增
      - alert: HighRiskAlertSpike
        expr: increase(high_risk_alerts_total[5m]) > 50
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "高风险告警激增"
          description: "5分钟内高风险告警增加了 {{ $value }} 个"
      
      # 数据库连接数过高
      - alert: DatabaseConnectionsHigh
        expr: mongodb_connections{state="current"} > 800
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "数据库连接数过高"
          description: "MongoDB连接数: {{ $value }}"
      
      # Redis内存使用率过高
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis内存使用率过高"
          description: "Redis内存使用率: {{ $value }}%"
```

**自动化运维脚本：**
```bash
#!/bin/bash
# scripts/auto_ops.sh

# 自动化运维脚本

set -e

# 配置
LOG_DIR="/var/log/compliance"
BACKUP_DIR="/backup/compliance"
MAX_LOG_DAYS=30
MAX_BACKUP_DAYS=7
ALERT_WEBHOOK="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"

# 日志函数
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_DIR/auto_ops.log"
}

# 发送告警
send_alert() {
    local message="$1"
    local level="${2:-warning}"
    
    curl -X POST "$ALERT_WEBHOOK" \
        -H 'Content-type: application/json' \
        --data "{
            \"text\": \"$message\"
            }]
        }"
log "告警已发送: $message"
}

# 清理日志文件
cleanup_logs() {
    log "开始清理过期日志文件..."
    
    find "$LOG_DIR" -name "*.log" -type f -mtime +$MAX_LOG_DAYS -delete
    
    # 压缩7天前的日志
    find "$LOG_DIR" -name "*.log" -type f -mtime +7 -exec gzip {} \;
    
    log "日志清理完成"
}

# 数据库备份
backup_database() {
    log "开始数据库备份..."
    
    local backup_file="$BACKUP_DIR/mongodb_$(date +%Y%m%d_%H%M%S).gz"
    
    # 创建备份目录
    mkdir -p "$BACKUP_DIR"
    
    # 执行MongoDB备份
    mongodump --host mongo-1:27017,mongo-2:27017,mongo-3:27017 \
              --db compliance \
              --gzip \
              --archive="$backup_file"
    
    if [ $? -eq 0 ]; then
        log "数据库备份成功: $backup_file"
    else
        log "数据库备份失败"
        send_alert "数据库备份失败" "critical"
        return 1
    fi
    
    # 清理过期备份
    find "$BACKUP_DIR" -name "mongodb_*.gz" -type f -mtime +$MAX_BACKUP_DAYS -delete
    
    log "备份清理完成"
}

# 系统健康检查
health_check() {
    log "开始系统健康检查..."
    
    local issues=0
    
    # 检查服务状态
    services=("web-backend" "website-detector" "traffic-analyzer")
    for service in "${services[@]}"; do
        if ! docker ps --filter "name=$service" --filter "status=running" | grep -q "$service"; then
            log "警告: 服务 $service 未运行"
            send_alert "服务 $service 未运行" "critical"
            ((issues++))
        fi
    done
    
    # 检查磁盘空间
    disk_usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    if [ "$disk_usage" -gt 85 ]; then
        log "警告: 磁盘使用率过高 ($disk_usage%)"
        send_alert "磁盘使用率过高: $disk_usage%" "warning"
        ((issues++))
    fi
    
    # 检查内存使用
    memory_usage=$(free | awk 'NR==2{printf "%.0f", $3*100/$2}')
    if [ "$memory_usage" -gt 90 ]; then
        log "警告: 内存使用率过高 ($memory_usage%)"
        send_alert "内存使用率过高: $memory_usage%" "warning"
        ((issues++))
    fi
    
    # 检查数据库连接
    if ! mongo --host mongo-1:27017 --eval "db.adminCommand('ping')" >/dev/null 2>&1; then
        log "错误: 无法连接到MongoDB"
        send_alert "无法连接到MongoDB" "critical"
        ((issues++))
    fi
    
    # 检查Redis连接
    if ! redis-cli -h redis-cluster ping >/dev/null 2>&1; then
        log "错误: 无法连接到Redis"
        send_alert "无法连接到Redis" "critical"
        ((issues++))
    fi
    
    if [ $issues -eq 0 ]; then
        log "系统健康检查通过"
    else
        log "系统健康检查发现 $issues 个问题"
    fi
    
    return $issues
}

# 性能优化
performance_optimization() {
    log "开始性能优化..."
    
    # 清理Docker未使用的资源
    docker system prune -f
    
    # 优化MongoDB索引
    mongo --host mongo-1:27017 compliance --eval "
        db.website_results.createIndex({url: 1, timestamp: -1});
        db.traffic_alerts.createIndex({timestamp: -1, risk_level: 1});
        db.detection_tasks.createIndex({status: 1, created_at: -1});
    "
    
    # 清理Redis过期键
    redis-cli -h redis-cluster --eval "return redis.call('FLUSHDB')" 0
    
    log "性能优化完成"
}

# 自动扩容检查
auto_scaling_check() {
    log "检查是否需要自动扩容..."
    
    # 检查CPU使用率
    cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
    
    # 检查队列长度
    queue_length=$(redis-cli -h redis-cluster llen "website_detection_queue")
    
    if (( $(echo "$cpu_usage > 80" | bc -l) )) && [ "$queue_length" -gt 500 ]; then
        log "触发自动扩容条件: CPU=$cpu_usage%, 队列长度=$queue_length"
        
        # 启动额外的检测服务实例
        docker-compose up -d --scale website-detector=5
        
        send_alert "已触发自动扩容: CPU=$cpu_usage%, 队列=$queue_length" "info"
    fi
}

# 主函数
main() {
    log "=== 自动化运维脚本开始执行 ==="
    
    # 创建必要的目录
    mkdir -p "$LOG_DIR" "$BACKUP_DIR"
    
    # 执行各项任务
    cleanup_logs
    backup_database
    health_check
    performance_optimization
    auto_scaling_check
    
    log "=== 自动化运维脚本执行完成 ==="
}

# 如果脚本被直接执行
if [ "${BASH_SOURCE[0]}" = "${0}" ]; then
    main "$@"
fi
```

## 阶段里程碑

### 里程碑检查点

- [ ] **分布式架构部署完成**
  - 多节点集群部署
  - 负载均衡配置
  - 数据库集群搭建
  - 服务发现和注册

- [ ] **机器学习模块集成完成**
  - 异常检测模型训练
  - 威胁分类模型部署
  - 智能告警过滤
  - 威胁情报生成

- [ ] **移动端应用发布**
  - iOS/Android应用开发
  - API接口完善
  - 推送通知功能
  - 离线数据缓存

- [ ] **监控告警体系完善**
  - Prometheus监控部署
  - Grafana仪表盘配置
  - 告警规则完善
  - 自动化运维脚本

### 验收标准

1. **性能验收**
   - 系统支持10万+网站并发检测
   - 流量分析处理能力达到10Gbps
   - API响应时间不超过500ms
   - 系统可用性达到99.9%

2. **功能验收**
   - 机器学习模型准确率达到90%以上
   - 移动端应用功能完整可用
   - 自动化运维脚本正常运行
   - 监控告警及时准确

3. **稳定性验收**
   - 分布式集群连续运行30天无故障
   - 自动扩容机制正常工作
   - 数据备份恢复流程验证通过
   - 故障自愈能力验证通过

## 技术难点与解决方案

### 1. 分布式系统一致性
**难点：** 多节点数据一致性保证
**解决方案：**
- 使用MongoDB副本集保证数据一致性
- Redis集群模式处理缓存
- 分布式锁机制避免并发冲突

### 2. 机器学习模型准确性
**难点：** 模型训练数据质量和准确性
**解决方案：**
- 建立标准化的数据标注流程
- 定期重训练和优化模型
- 多模型集成提高准确性

### 3. 系统性能瓶颈
**难点：** 大规模并发处理性能优化
**解决方案：**
- 异步处理和消息队列
- 数据库读写分离
- CDN和缓存优化

## 运维最佳实践

### 1. 监控策略
- **基础监控**：CPU、内存、磁盘、网络
- **应用监控**：API响应时间、错误率、吞吐量
- **业务监控**：检测任务数、告警数量、处理效率

### 2. 告警策略
- **分级告警**：critical、warning、info三个级别
- **告警聚合**：避免告警风暴
- **告警抑制**：维护期间暂停告警

### 3. 备份策略
- **数据备份**：每日全量备份，每小时增量备份
- **配置备份**：版本控制管理配置文件
- **镜像备份**：定期备份Docker镜像

### 4. 安全策略
- **网络安全**：VPN访问，防火墙规则
- **数据安全**：数据加密，访问控制
- **应用安全**：定期安全扫描，漏洞修复

## 成本优化建议

### 1. 资源优化
- **弹性伸缩**：根据负载自动调整资源
- **资源池化**：共享计算和存储资源
- **定时任务**：非高峰期执行重型任务

### 2. 云服务优化
- **预留实例**：长期使用的服务购买预留实例
- **Spot实例**：非关键任务使用Spot实例
- **多云策略**：避免供应商锁定

### 3. 开发效率优化
- **CI/CD流水线**：自动化部署减少人工成本
- **代码复用**：组件化开发提高效率
- **文档完善**：减少沟通成本

## 项目总结

通过三个阶段的迭代开发，内容合规检测系统已经从MVP产品发展为企业级平台：

### 核心成果
1. **技术架构成熟**：分布式、高可用、可扩展
2. **功能覆盖全面**：网站检测、流量分析、智能告警
3. **用户体验优秀**：Web管理后台、移动端应用
4. **运维体系完善**：监控告警、自动化运维

### 技术亮点
1. **机器学习加持**：智能检测，减少误报
2. **微服务架构**：模块独立，易于维护
3. **容器化部署**：环境一致，快速部署
4. **实时处理**：秒级响应，及时告警

### 业务价值
1. **合规风险降低**：自动化检测，全面覆盖
2. **运营效率提升**：智能告警，精准处置
3. **成本控制优化**：自动化运维，减少人力
4. **决策支持增强**：数据分析，趋势预测

### 未来展望
1. **AI能力增强**：深度学习，更智能的检测
2. **生态集成扩展**：对接更多第三方服务
3. **国际化支持**：多语言，多地区部署
4. **行业解决方案**：针对特定行业定制化

## 内存安全优化措施

### 1. 企业级内存管理
```python
# 企业级内存管理器
class EnterpriseMemoryManager:
    def __init__(self):
        self.memory_pools = {}
        self.memory_monitors = {}
        self.alert_system = AlertSystem()
    
    def create_memory_pool(self, pool_name: str, max_size_mb: int):
        """创建内存池"""
        self.memory_pools[pool_name] = {
            'max_size': max_size_mb * 1024 * 1024,  # 转换为字节
            'current_usage': 0,
            'allocations': {},
            'lock': threading.Lock()
        }
    
    def allocate_memory(self, pool_name: str, size: int, purpose: str) -> str:
        """从内存池分配内存"""
        if pool_name not in self.memory_pools:
            raise ValueError(f"内存池 {pool_name} 不存在")
        
        pool = self.memory_pools[pool_name]
        
        with pool['lock']:
            if pool['current_usage'] + size > pool['max_size']:
                # 内存不足，尝试清理
                self.cleanup_pool(pool_name)
                
                if pool['current_usage'] + size > pool['max_size']:
                    raise MemoryError(f"内存池 {pool_name} 空间不足")
            
            # 分配内存
            allocation_id = f"{pool_name}_{int(time.time() * 1000)}"
            pool['allocations'][allocation_id] = {
                'size': size,
                'purpose': purpose,
                'allocated_at': time.time()
            }
            pool['current_usage'] += size
            
            return allocation_id
    
    def free_memory(self, pool_name: str, allocation_id: str):
        """释放内存"""
        if pool_name not in self.memory_pools:
            return
        
        pool = self.memory_pools[pool_name]
        
        with pool['lock']:
            if allocation_id in pool['allocations']:
                size = pool['allocations'][allocation_id]['size']
                pool['current_usage'] -= size
                del pool['allocations'][allocation_id]
    
    def cleanup_pool(self, pool_name: str):
        """清理内存池"""
        pool = self.memory_pools[pool_name]
        
        # 清理过期的分配
        current_time = time.time()
        expired_allocations = []
        
        for alloc_id, alloc_info in pool['allocations'].items():
            if current_time - alloc_info['allocated_at'] > 3600:  # 1小时过期
                expired_allocations.append(alloc_id)
        
        for alloc_id in expired_allocations:
            self.free_memory(pool_name, alloc_id)
    
    def get_pool_status(self, pool_name: str) -> dict:
        """获取内存池状态"""
        if pool_name not in self.memory_pools:
            return {}
        
        pool = self.memory_pools[pool_name]
        usage_percent = (pool['current_usage'] / pool['max_size']) * 100
        
        return {
            'current_usage_mb': pool['current_usage'] / 1024 / 1024,
            'max_size_mb': pool['max_size'] / 1024 / 1024,
            'usage_percent': usage_percent,
            'allocation_count': len(pool['allocations']),
            'status': 'warning' if usage_percent > 80 else 'normal'
        }
```

### 2. 智能内存预测
```python
# 智能内存预测器
class MemoryPredictor:
    def __init__(self):
        self.historical_data = []
        self.prediction_model = None
    
    def collect_memory_data(self, memory_usage: float, timestamp: float):
        """收集内存使用数据"""
        self.historical_data.append({
            'usage': memory_usage,
            'timestamp': timestamp
        })
        
        # 保持最近24小时的数据
        cutoff_time = timestamp - 86400
        self.historical_data = [
            data for data in self.historical_data 
            if data['timestamp'] > cutoff_time
        ]
    
    def predict_memory_usage(self, minutes_ahead: int = 30) -> float:
        """预测未来内存使用"""
        if len(self.historical_data) < 10:
            return 0.0
        
        # 简单的线性回归预测
        recent_data = self.historical_data[-10:]
        x = [data['timestamp'] for data in recent_data]
        y = [data['usage'] for data in recent_data]
        
        # 计算趋势
        if len(x) > 1:
            slope = (y[-1] - y[0]) / (x[-1] - x[0])
            current_usage = y[-1]
            predicted_usage = current_usage + slope * (minutes_ahead * 60)
            
            return max(0.0, min(100.0, predicted_usage))
        
        return y[-1] if y else 0.0
    
    def should_scale_up(self) -> bool:
        """判断是否需要扩容"""
        predicted_usage = self.predict_memory_usage(30)
        return predicted_usage > 80.0
```

### 3. 自动内存优化
```python
# 自动内存优化器
class AutoMemoryOptimizer:
    def __init__(self):
        self.memory_manager = EnterpriseMemoryManager()
        self.predictor = MemoryPredictor()
        self.optimization_rules = []
    
    def add_optimization_rule(self, rule: dict):
        """添加优化规则"""
        self.optimization_rules.append(rule)
    
    def optimize_memory(self):
        """执行内存优化"""
        current_usage = psutil.Process().memory_percent()
        predicted_usage = self.predictor.predict_memory_usage()
        
        # 检查是否需要优化
        if current_usage > 80.0 or predicted_usage > 85.0:
            self.execute_optimization_strategies()
    
    def execute_optimization_strategies(self):
        """执行优化策略"""
        strategies = [
            self.cleanup_expired_cache,
            self.compress_memory_data,
            self.evict_least_used_data,
            self.scale_down_services
        ]
        
        for strategy in strategies:
            try:
                strategy()
            except Exception as e:
                logger.error(f"优化策略执行失败: {e}")
    
    def cleanup_expired_cache(self):
        """清理过期缓存"""
        # 清理Redis过期数据
        redis_client.flushdb()
        
        # 清理本地缓存
        for cache in self.get_local_caches():
            cache.cleanup_expired()
    
    def compress_memory_data(self):
        """压缩内存数据"""
        # 压缩存储的数据
        for pool_name in self.memory_manager.memory_pools:
            self.compress_pool_data(pool_name)
    
    def evict_least_used_data(self):
        """淘汰最少使用的数据"""
        # 实现LRU淘汰策略
        for pool_name in self.memory_manager.memory_pools:
            self.evict_lru_data(pool_name)
    
    def scale_down_services(self):
        """缩减服务规模"""
        # 减少并发数
        self.reduce_concurrency()
        
        # 暂停非关键服务
        self.pause_non_critical_services()
```

### 4. 内存监控告警
```python
# 企业级内存监控
class EnterpriseMemoryMonitor:
    def __init__(self):
        self.monitors = {}
        self.alert_thresholds = {
            'warning': 70.0,
            'critical': 85.0,
            'emergency': 95.0
        }
    
    def add_monitor(self, name: str, monitor_config: dict):
        """添加监控器"""
        self.monitors[name] = {
            'config': monitor_config,
            'last_check': 0,
            'status': 'normal'
        }
    
    def check_all_monitors(self):
        """检查所有监控器"""
        for name, monitor in self.monitors.items():
            try:
                status = self.check_monitor(name, monitor)
                if status != monitor['status']:
                    self.handle_status_change(name, monitor['status'], status)
                    monitor['status'] = status
            except Exception as e:
                logger.error(f"监控器 {name} 检查失败: {e}")
    
    def check_monitor(self, name: str, monitor: dict) -> str:
        """检查单个监控器"""
        current_usage = psutil.Process().memory_percent()
        
        if current_usage > self.alert_thresholds['emergency']:
            return 'emergency'
        elif current_usage > self.alert_thresholds['critical']:
            return 'critical'
        elif current_usage > self.alert_thresholds['warning']:
            return 'warning'
        else:
            return 'normal'
    
    def handle_status_change(self, monitor_name: str, old_status: str, new_status: str):
        """处理状态变化"""
        if new_status in ['warning', 'critical', 'emergency']:
            self.send_alert(monitor_name, new_status)
        
        if new_status == 'emergency':
            self.trigger_emergency_response()
    
    def send_alert(self, monitor_name: str, status: str):
        """发送告警"""
        alert_data = {
            'monitor': monitor_name,
            'status': status,
            'memory_usage': psutil.Process().memory_percent(),
            'timestamp': time.time()
        }
        
        # 发送到告警系统
        self.alert_system.send_alert(alert_data)
    
    def trigger_emergency_response(self):
        """触发紧急响应"""
        # 立即执行紧急优化
        optimizer = AutoMemoryOptimizer()
        optimizer.execute_emergency_optimization()
        
        # 通知运维团队
        self.notify_operations_team()
```

### 5. 风险控制措施
- **预测性监控**：基于历史数据预测内存使用趋势
- **自动优化**：根据预测结果自动执行优化策略
- **分级告警**：不同级别的内存告警触发不同响应
- **紧急响应**：内存严重不足时的紧急处理机制
- **资源隔离**：关键服务的内存隔离保护