# 第一阶段：核心功能开发（1-2个月）

## 阶段目标

### 总体目标
实现基础的网站合规检测和告警功能，构建系统的核心能力，为后续功能扩展奠定坚实基础。

### 详细目标

#### 1. 技术架构目标
- **建立MVP架构**：构建最小可行产品架构，包含用户界面层、核心服务层、数据存储层
- **模块化设计**：实现5个核心模块的独立开发、测试和部署
- **技术栈验证**：验证FastAPI、MongoDB、Redis、Celery等技术栈的可行性
- **容器化部署**：所有模块支持Docker容器化，实现一键部署
- **内存安全防护**：建立完善的内存管理和资源清理机制，防止OOM和内存泄漏
- **并发安全控制**：实现安全的并发访问控制，防止资源竞争和数据不一致

#### 2. 功能实现目标
- **网站检测能力**：
  - 支持单域名网站内容爬取（文本+图片）
  - 集成第三方内容审核API（百度AI/阿里云）
  - 实现备案信息查询功能
  - 检测结果存储和查询
- **告警处置能力**：
  - 多级告警机制（高/中/低风险）
  - 多渠道通知（邮件+短信）
  - 基础自动处置功能
- **任务调度能力**：
  - 定时批量扫描任务
  - 分布式任务队列
  - 任务状态监控和重试
- **配置管理能力**：
  - 系统参数配置
  - 用户权限管理
  - 检测规则配置
- **Web管理界面**：
  - 用户登录认证
  - 扫描任务管理
  - 告警事件查看
  - 基础系统配置

#### 3. 性能指标目标
- **检测性能**：单个网站检测时间≤30秒
- **并发能力**：支持10个网站并发检测
- **告警延迟**：告警通知延迟≤5分钟
- **系统稳定性**：连续运行24小时无崩溃
- **数据处理**：支持1000个网站的检测结果存储
- **内存安全**：单个爬取任务内存使用≤500MB，系统内存使用率≤80%
- **资源管理**：浏览器实例正确释放，数据库连接池有效管理

#### 4. 质量保证目标
- **代码质量**：所有模块包含单元测试，测试覆盖率≥80%
- **API规范**：遵循RESTful设计原则，统一错误码规范
- **文档完整**：每个模块包含API文档和部署文档
- **安全性**：实现基础的认证授权机制
- **内存安全**：建立内存泄漏检测机制，定期资源清理，防止OOM
- **并发安全**：实现线程安全的并发处理，避免数据竞争

#### 5. 业务验证目标
- **合规检测准确性**：文本检测准确率≥90%，图片检测准确率≥85%
- **误报控制**：误报率≤10%
- **用户体验**：Web界面响应时间≤3秒
- **运维友好**：支持日志查看、状态监控、配置热更新

## 开发顺序

1. **配置管理模块**（基础设施）
2. **网站检测模块**（核心功能）
3. **告警处置模块**（核心功能）
4. **任务调度模块**（支撑功能）
5. **Web管理后台**（基础界面）

## 详细模块设计

### 1. 配置管理模块 (config-manager)

**功能职责：**
- 系统配置管理
- 检测规则配置
- 用户权限管理
- 系统参数调优

**技术栈：**
- FastAPI (Web框架)
- Pydantic (配置验证)
- PyYAML (配置文件)
- SQLAlchemy (用户管理)

**目录结构：**
```
config-manager/
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI应用入口
│   ├── models/
│   │   ├── __init__.py
│   │   ├── config.py           # 配置模型
│   │   └── user.py             # 用户模型
│   ├── services/
│   │   ├── __init__.py
│   │   ├── config_service.py   # 配置服务
│   │   └── user_service.py     # 用户服务
│   ├── api/
│   │   ├── __init__.py
│   │   ├── config.py           # 配置API
│   │   ├── users.py            # 用户API
│   │   └── auth.py             # 认证API
│   └── schemas/
│       ├── __init__.py
│       ├── config_schema.py    # 配置Schema
│       └── user_schema.py      # 用户Schema
├── configs/
│   ├── default.yaml            # 默认配置
│   ├── production.yaml         # 生产环境配置
│   └── development.yaml        # 开发环境配置
├── tests/
├── requirements.txt
└── Dockerfile
```

**开发优先级：** 高（基础设施）
**预估工期：** 2-3周

**核心功能实现：**

```python
# config_service.py 示例代码
from typing import Dict, Any
import yaml
from pathlib import Path

class ConfigService:
    def __init__(self, config_path: str = "configs/default.yaml"):
        self.config_path = Path(config_path)
        self._config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """加载配置文件"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """获取配置项"""
        keys = key.split('.')
        value = self._config
        for k in keys:
            value = value.get(k, default)
            if value is None:
                return default
        return value
    
    def update_config(self, key: str, value: Any) -> bool:
        """更新配置项"""
        keys = key.split('.')
        config = self._config
        for k in keys[:-1]:
            config = config.setdefault(k, {})
        config[keys[-1]] = value
        return self._save_config()
    
    def _save_config(self) -> bool:
        """保存配置到文件"""
        try:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(self._config, f, default_flow_style=False, allow_unicode=True)
            return True
        except Exception as e:
            print(f"保存配置失败: {e}")
            return False
```

### 2. 网站检测模块 (website-scanner)

**功能职责：**
- 网站内容爬取
- 文本/图片合规检测
- 备案信息查询
- 检测结果存储

**技术栈：**
- FastAPI (Web框架)
- Scrapy + Playwright (爬虫)
- Requests (HTTP客户端)
- PyMongo (MongoDB驱动)

**目录结构：**
```
website-scanner/
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI应用入口
│   ├── models/
│   │   ├── __init__.py
│   │   ├── website.py          # 网站数据模型
│   │   └── scan_result.py      # 检测结果模型
│   ├── services/
│   │   ├── __init__.py
│   │   ├── crawler.py          # 爬虫服务
│   │   ├── content_checker.py  # 内容检测服务
│   │   └── beian_checker.py    # 备案查询服务
│   ├── api/
│   │   ├── __init__.py
│   │   ├── scan.py             # 扫描API
│   │   └── results.py          # 结果查询API
│   └── utils/
│       ├── __init__.py
│       ├── database.py         # 数据库连接
│       └── config.py           # 配置管理
├── tests/
├── requirements.txt
└── Dockerfile
```

**开发优先级：** 高（核心功能）
**预估工期：** 3-4周

**核心功能实现：**

```python
# website_scanner.py 核心逻辑
import requests
from playwright.sync_api import sync_playwright
from pymongo import MongoClient
import time
from typing import Dict, List, Optional

class WebsiteScanner:
    def __init__(self, api_key: str, db_url: str = "mongodb://localhost:27017/"):
        self.content_api = "https://api.xxx.com/content/scan"
        self.api_key = api_key
        self.db = MongoClient(db_url)["compliance"]
    
    def crawl_website(self, domain: str, max_pages: int = 10) -> Optional[Dict]:
        """爬取网站内容（首页+随机内页）"""
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            try:
                page.goto(f"https://{domain}", timeout=10000)
                time.sleep(2)  # 等待页面加载
                
                # 提取页面内容
                text_content = page.inner_text("body")
                image_urls = [img.get_attribute("src") for img in page.query_selector_all("img")]
                
                return {
                    "text": text_content,
                    "images": [url for url in image_urls if url and url.startswith("http")][:5]
                }
            except Exception as e:
                print(f"爬取失败: {domain}, 错误: {e}")
                return None
            finally:
                browser.close()
    
    def check_compliance(self, domain: str) -> Dict:
        """检测网站合规性"""
        website_data = self.crawl_website(domain)
        if not website_data:
            return {"status": "error", "message": "爬取失败"}
        
        # 1. 文本合规检测
        text_result = self._check_text(website_data["text"])
        
        # 2. 图片合规检测
        image_results = []
        for img_url in website_data["images"]:
            img_result = self._check_image(img_url)
            image_results.append(img_result)
        
        # 3. 汇总结果
        is_compliant = all([
            text_result.get("compliant", False),
            all([r.get("compliant", False) for r in image_results])
        ])
        
        result = {
            "domain": domain,
            "compliant": is_compliant,
            "text_result": text_result,
            "image_results": image_results,
            "timestamp": time.time()
        }
        
        # 存储结果到数据库
        self.db.website_checks.insert_one(result)
        return result
    
    def _check_text(self, text: str) -> Dict:
        """调用第三方文本审核API"""
        try:
            response = requests.post(
                self.content_api,
                json={"text": text, "type": "text"},
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=30
            )
            return response.json()
        except Exception as e:
            return {"compliant": False, "error": str(e)}
    
    def _check_image(self, image_url: str) -> Dict:
        """调用第三方图片审核API"""
        try:
            response = requests.post(
                self.content_api,
                json={"url": image_url, "type": "image"},
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=30
            )
            return response.json()
        except Exception as e:
            return {"compliant": False, "error": str(e)}
```

### 3. 告警处置模块 (alert-handler)

**功能职责：**
- 告警事件接收
- 告警级别判定
- 多渠道通知发送
- 自动处置执行
- 工单系统集成

**技术栈：**
- FastAPI (Web框架)
- Celery (异步任务)
- SMTP/SMS (通知渠道)
- Requests (外部API调用)

**目录结构：**
```
alert-handler/
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI应用入口
│   ├── models/
│   │   ├── __init__.py
│   │   ├── alert.py            # 告警模型
│   │   └── action.py           # 处置动作模型
│   ├── services/
│   │   ├── __init__.py
│   │   ├── alert_processor.py  # 告警处理服务
│   │   ├── notification.py     # 通知服务
│   │   ├── auto_action.py      # 自动处置服务
│   │   └── ticket_system.py    # 工单系统集成
│   ├── api/
│   │   ├── __init__.py
│   │   ├── alerts.py           # 告警API
│   │   └── actions.py          # 处置API
│   └── tasks/
│       ├── __init__.py
│       ├── email_tasks.py      # 邮件任务
│       ├── sms_tasks.py        # 短信任务
│       └── action_tasks.py     # 处置任务
├── templates/
│   ├── email_alert.html        # 邮件模板
│   └── sms_alert.txt           # 短信模板
├── tests/
├── requirements.txt
└── Dockerfile
```

**开发优先级：** 高（核心功能）
**预估工期：** 2-3周

### 4. 任务调度模块 (task-scheduler)

**功能职责：**
- 定时任务管理
- 分布式任务调度
- 任务状态监控
- 失败重试机制

**技术栈：**
- Celery (任务队列)
- RabbitMQ (消息代理)
- Redis (结果存储)
- Flower (监控界面)

**目录结构：**
```
task-scheduler/
├── app/
│   ├── __init__.py
│   ├── celery_app.py           # Celery应用配置
│   ├── tasks/
│   │   ├── __init__.py
│   │   ├── scan_tasks.py       # 扫描任务
│   │   ├── report_tasks.py     # 报表任务
│   │   └── cleanup_tasks.py    # 清理任务
│   ├── schedules/
│   │   ├── __init__.py
│   │   └── periodic_tasks.py   # 周期性任务配置
│   └── utils/
│       ├── __init__.py
│       └── task_utils.py       # 任务工具函数
├── config/
│   ├── celery_config.py        # Celery配置
│   └── rabbitmq_config.py      # RabbitMQ配置
├── scripts/
│   ├── start_worker.sh         # 启动Worker脚本
│   └── start_beat.sh           # 启动Beat脚本
├── tests/
├── requirements.txt
└── docker-compose.yml
```

**开发优先级：** 中（支撑功能）
**预估工期：** 2周

### 5. Web管理后台 (web-admin) - 基础版本

**功能职责：**
- 基础系统管理界面
- 检测任务管理
- 告警事件查看
- 基础配置设置

**技术栈：**
- Vue.js 3 (前端框架)
- Element Plus (UI组件库)
- Axios (HTTP客户端)
- Echarts (图表库)

**第一阶段重点功能：**
- 用户登录/认证
- 扫描任务创建和管理
- 告警事件列表和详情
- 基础系统配置

**开发优先级：** 中（用户界面）
**预估工期：** 3-4周

## 阶段里程碑

### 里程碑检查点

- [ ] **配置管理模块完成**
  - 系统配置CRUD功能
  - 用户认证和权限管理
  - 配置文件热加载

- [ ] **网站检测模块完成**
  - 单域名网站内容检测
  - 第三方API集成（文本/图片审核）
  - 检测结果存储和查询

- [ ] **告警处置模块完成**
  - 邮件/短信告警通知
  - 告警级别自动判定
  - 基础自动处置功能

- [ ] **任务调度模块完成**
  - 定时批量扫描任务
  - 任务状态监控
  - 失败重试机制

- [ ] **Web管理后台完成**
  - 基础管理界面
  - 扫描任务管理
  - 告警事件查看

### 验收标准

1. **功能验收**
   - 能够成功扫描指定域名网站
   - 检测到违规内容时能及时发送告警
   - 支持批量扫描任务的定时执行
   - Web界面能正常管理系统功能

2. **性能验收**
   - 单个网站扫描时间不超过30秒
   - 告警通知延迟不超过5分钟
   - 系统支持并发处理10个扫描任务

3. **稳定性验收**
   - 系统连续运行24小时无崩溃
   - 网络异常时能正常重试和恢复
   - 数据库连接异常时能自动重连

## 技术规范

### 代码规范
- Python代码遵循PEP 8规范
- JavaScript代码遵循ESLint标准
- 所有模块必须包含单元测试
- API接口遵循RESTful设计原则

### 数据库设计
- MongoDB用于存储检测结果
- Redis用于缓存和任务队列
- 数据库连接池配置

### 接口设计
- 统一使用JSON格式进行数据交换
- API版本控制采用URL路径方式（/api/v1/）
- 错误响应遵循统一的错误码规范

### 部署规范
- 所有模块支持Docker容器化部署
- 使用docker-compose进行本地开发环境搭建

## 风险控制

### 技术风险
1. **第三方API限制**
   - 风险：内容审核API调用频率限制
   - 应对：实现API调用频率控制，准备备用API

2. **爬虫反爬机制**
   - 风险：目标网站反爬虫机制
   - 应对：使用代理IP池，模拟真实浏览器行为

### 进度风险
1. **开发进度延期**
   - 风险：模块开发时间超出预期
   - 应对：每周进度检查，及时调整资源分配

2. **集成测试问题**
   - 风险：模块间集成出现问题
   - 应对：提前进行接口联调，建立集成测试环境

## 下一阶段准备

### 为第二阶段做准备
1. **架构优化**
   - 评估系统性能瓶颈
   - 设计分布式部署方案

2. **功能扩展**
   - 调研流量分析技术方案
   - 准备子域名扫描算法

3. **团队建设**
   - 总结第一阶段开发经验
   - 制定第二阶段开发计划

## 内存安全防护措施

### 1. 浏览器实例管理
```python
# 浏览器实例池管理 - 防止内存泄漏
class BrowserPool:
    def __init__(self, max_browsers=5):
        self.max_browsers = max_browsers
        self.active_browsers = []
        self.lock = asyncio.Lock()
    
    async def get_browser(self):
        async with self.lock:
            if len(self.active_browsers) >= self.max_browsers:
                await self.wait_for_available()
            
            browser = await self.create_browser()
            self.active_browsers.append(browser)
            return browser
    
    async def release_browser(self, browser):
        async with self.lock:
            if browser in self.active_browsers:
                await browser.close()
                self.active_browsers.remove(browser)
    
    async def cleanup_all(self):
        async with self.lock:
            for browser in self.active_browsers:
                try:
                    await browser.close()
                except:
                    pass
            self.active_browsers.clear()
```

### 2. 内存监控告警
```python
# 内存监控服务
class MemoryMonitor:
    def __init__(self):
        self.process = psutil.Process()
        self.warning_threshold = 80.0
        self.critical_threshold = 90.0
    
    def get_memory_usage(self) -> float:
        return self.process.memory_percent()
    
    def check_memory_status(self) -> dict:
        usage = self.get_memory_usage()
        return {
            'usage': usage,
            'status': 'critical' if usage > self.critical_threshold else 'warning' if usage > self.warning_threshold else 'normal',
            'warning': usage > self.warning_threshold
        }
    
    async def monitor_memory(self):
        """持续监控内存使用"""
        while True:
            status = self.check_memory_status()
            if status['warning']:
                await self.trigger_memory_alert(status)
            await asyncio.sleep(30)
```

### 3. 资源清理机制
```python
# 定期清理任务
@celery.task
def cleanup_resources():
    """定期清理资源"""
    try:
        # 清理浏览器实例
        browser_pool.cleanup_expired()
        
        # 清理数据库连接
        db_pool.cleanup_expired()
        
        # 清理缓存
        cache_manager.cleanup_expired()
        
        # 强制垃圾回收
        gc.collect()
        
        logger.info("资源清理完成")
    except Exception as e:
        logger.error(f"资源清理失败: {e}")

# 定时执行清理任务
@celery.on_after_configure.connect
def setup_periodic_tasks(sender, **kwargs):
    sender.add_periodic_task(
        crontab(minute='*/5'),  # 每5分钟执行一次
        cleanup_resources.s(),
        name='cleanup-resources'
    )
```

### 4. 并发安全控制
```python
# 并发控制配置
CONCURRENT_CONFIG = {
    "max_concurrent_scans": 5,      # 最大并发扫描数
    "max_browser_instances": 3,     # 最大浏览器实例数
    "semaphore_timeout": 30,        # 信号量超时时间
    "connection_pool_size": 10,     # 数据库连接池大小
    "redis_connection_limit": 20    # Redis连接限制
}

# 信号量控制
scan_semaphore = asyncio.Semaphore(CONCURRENT_CONFIG["max_concurrent_scans"])
browser_semaphore = asyncio.Semaphore(CONCURRENT_CONFIG["max_browser_instances"])

async def safe_scan_website(domain: str):
    """安全的网站扫描"""
    async with scan_semaphore:
        async with browser_semaphore:
            # 执行扫描逻辑
            return await perform_scan(domain)
```

### 5. 配置优化
```python
# 内存安全配置
MEMORY_SAFE_CONFIG = {
    "browser_max_memory_mb": 512,        # 单个浏览器最大内存
    "browser_timeout": 30,               # 浏览器超时时间
    "page_timeout": 10,                  # 页面加载超时
    "memory_check_interval": 5,          # 内存检查间隔
    "force_gc_threshold": 80,            # 强制垃圾回收阈值
    "max_text_length": 10000,            # 最大文本长度
    "max_images_per_site": 5,            # 每个站点最大图片数
    "cache_ttl": 3600,                   # 缓存过期时间
    "connection_timeout": 30,            # 连接超时时间
}
```

### 6. 风险控制措施
- **内存泄漏检测**：定期检查内存使用情况，超过阈值自动告警
- **资源自动清理**：定时清理过期资源，强制垃圾回收
- **并发限制**：严格控制并发数量，防止资源耗尽
- **异常处理**：完善的异常处理机制，确保资源正确释放
- **监控告警**：实时监控系统资源使用，及时发现问题