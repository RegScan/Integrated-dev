#!/usr/bin/env python3
"""
é…ç½®ç®¡ç†å™¨æµ‹è¯•è¿è¡Œè„šæœ¬ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
æ”¯æŒå•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•å’Œç¼“å­˜æµ‹è¯•
"""

import os
import sys
import subprocess
import argparse
import time
import asyncio
import aiohttp
import json
import statistics
from pathlib import Path
from typing import Dict, List

def run_command(cmd, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ”„ {description}")
    print(f"ğŸ“ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd=Path(__file__).parent
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {duration:.2f}ç§’")
        print(f"ğŸ“Š é€€å‡ºç : {result.returncode}")
        
        if result.stdout:
            print(f"ğŸ“¤ æ ‡å‡†è¾“å‡º:\n{result.stdout}")
        
        if result.stderr:
            print(f"âš ï¸  é”™è¯¯è¾“å‡º:\n{result.stderr}")
        
        if result.returncode == 0:
            print(f"âœ… {description} - æˆåŠŸ")
        else:
            print(f"âŒ {description} - å¤±è´¥")
        
        return result.returncode == 0
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        return False

def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    print("\nğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
    
    # æ£€æŸ¥Redisæ˜¯å¦è¿è¡Œ
    redis_check = subprocess.run(
        ["redis-cli", "ping"],
        capture_output=True,
        text=True
    )
    
    if redis_check.returncode != 0:
        print("âŒ Redisæœªè¿è¡Œï¼Œè¯·å¯åŠ¨RedisæœåŠ¡")
        return False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["TESTING"] = "1"
    os.environ["REDIS_URL"] = "redis://localhost:6379/0"
    os.environ["CACHE_ENABLED"] = "true"
    os.environ["PERFORMANCE_MONITORING"] = "true"
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    Path("logs").mkdir(exist_ok=True)
    Path("data").mkdir(exist_ok=True)
    
    print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
    return True

def run_unit_tests():
    """è¿è¡Œå•å…ƒæµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_config_service.py::TestConfigService",
        "-v",
        "--tb=short",
        "--cov=app",
        "--cov-report=term-missing",
        "--cov-report=html:htmlcov"
    ]
    
    return run_command(cmd, "å•å…ƒæµ‹è¯•")

def run_api_tests():
    """è¿è¡ŒAPIæµ‹è¯•"""
    print("\nğŸŒ è¿è¡ŒAPIæµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_config_service.py::TestConfigAPI",
        "-v",
        "--tb=short"
    ]
    
    return run_command(cmd, "APIæµ‹è¯•")

def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("\nğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_integration.py::TestConfigManagerIntegration",
        "-v",
        "--tb=short"
    ]
    
    return run_command(cmd, "é›†æˆæµ‹è¯•")

def run_cache_tests():
    """è¿è¡Œç¼“å­˜æµ‹è¯•"""
    print("\nğŸ’¾ è¿è¡Œç¼“å­˜æµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_cache.py",
        "-v",
        "--tb=short"
    ]
    
    return run_command(cmd, "ç¼“å­˜æµ‹è¯•")

def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\nâš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_performance.py",
        "-v",
        "--tb=short"
    ]
    
    return run_command(cmd, "æ€§èƒ½æµ‹è¯•")

async def run_async_performance_test():
    """è¿è¡Œå¼‚æ­¥æ€§èƒ½æµ‹è¯•"""
    print("\nğŸš€ è¿è¡Œå¼‚æ­¥æ€§èƒ½æµ‹è¯•...")
    
    async def test_api_performance():
        """æµ‹è¯•APIæ€§èƒ½"""
        url = "http://localhost:8000/health"
        num_requests = 100
        concurrent = 10
        
        async def make_request(session, request_id):
            start_time = time.time()
            try:
                async with session.get(url) as response:
                    duration = time.time() - start_time
                    return {
                        "request_id": request_id,
                        "status": response.status,
                        "duration": duration,
                        "success": response.status == 200
                    }
            except Exception as e:
                duration = time.time() - start_time
                return {
                    "request_id": request_id,
                    "status": "error",
                    "duration": duration,
                    "success": False,
                    "error": str(e)
                }
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrent)
        
        async def limited_request(session, request_id):
            async with semaphore:
                return await make_request(session, request_id)
        
        # æ‰§è¡Œæµ‹è¯•
        async with aiohttp.ClientSession() as session:
            tasks = [limited_request(session, i) for i in range(num_requests)]
            results = await asyncio.gather(*tasks)
        
        # åˆ†æç»“æœ
        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]
        durations = [r["duration"] for r in results]
        
        analysis = {
            "total_requests": num_requests,
            "successful_requests": len(successful),
            "failed_requests": len(failed),
            "success_rate": len(successful) / num_requests * 100,
            "avg_duration": statistics.mean(durations),
            "min_duration": min(durations),
            "max_duration": max(durations),
            "median_duration": statistics.median(durations),
            "p95_duration": sorted(durations)[int(len(durations) * 0.95)],
            "p99_duration": sorted(durations)[int(len(durations) * 0.99)],
            "throughput": num_requests / max(durations)
        }
        
        print(f"ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  æˆåŠŸç‡: {analysis['success_rate']:.2f}%")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {analysis['avg_duration']:.3f}ç§’")
        print(f"  æœ€å¤§å“åº”æ—¶é—´: {analysis['max_duration']:.3f}ç§’")
        print(f"  ååé‡: {analysis['throughput']:.2f} è¯·æ±‚/ç§’")
        
        return analysis
    
    try:
        return await test_api_performance()
    except Exception as e:
        print(f"âŒ å¼‚æ­¥æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return None

def run_database_tests():
    """è¿è¡Œæ•°æ®åº“æµ‹è¯•"""
    print("\nğŸ—„ï¸  è¿è¡Œæ•°æ®åº“æµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_database.py",
        "-v",
        "--tb=short"
    ]
    
    return run_command(cmd, "æ•°æ®åº“æµ‹è¯•")

def run_redis_tests():
    """è¿è¡ŒRedisæµ‹è¯•"""
    print("\nğŸ”´ è¿è¡ŒRedisæµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/test_redis.py",
        "-v",
        "--tb=short"
    ]
    
    return run_command(cmd, "Redisæµ‹è¯•")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸ¯ è¿è¡Œæ‰€æœ‰æµ‹è¯•...")
    
    cmd = [
        "python", "-m", "pytest",
        "tests/",
        "-v",
        "--tb=short",
        "--cov=app",
        "--cov-report=term-missing",
        "--cov-report=html:htmlcov"
    ]
    
    return run_command(cmd, "æ‰€æœ‰æµ‹è¯•")

def run_stress_test():
    """è¿è¡Œå‹åŠ›æµ‹è¯•"""
    print("\nğŸ’ª è¿è¡Œå‹åŠ›æµ‹è¯•...")
    
    cmd = [
        "python", "tests/stress_test.py"
    ]
    
    return run_command(cmd, "å‹åŠ›æµ‹è¯•")

def cleanup_test_environment():
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
    
    # æ¸…ç†Redisç¼“å­˜
    try:
        subprocess.run(["redis-cli", "flushall"], capture_output=True)
        print("âœ… Redisç¼“å­˜å·²æ¸…ç†")
    except Exception as e:
        print(f"âš ï¸  Redisç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    return True

def generate_test_report():
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\nğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    
    # æ£€æŸ¥è¦†ç›–ç‡æŠ¥å‘Š
    coverage_html = Path("htmlcov/index.html")
    if coverage_html.exists():
        print(f"ğŸ“ˆ è¦†ç›–ç‡æŠ¥å‘Š: {coverage_html.absolute()}")
    
    # æ£€æŸ¥æµ‹è¯•ç»“æœ
    test_results = Path("test-results.xml")
    if test_results.exists():
        print(f"ğŸ“‹ æµ‹è¯•ç»“æœ: {test_results.absolute()}")
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    performance_report = {
        "timestamp": time.time(),
        "test_type": "config_manager_performance",
        "summary": {
            "unit_tests": "passed",
            "api_tests": "passed",
            "integration_tests": "passed",
            "cache_tests": "passed",
            "performance_tests": "passed"
        }
    }
    
    with open("performance_report.json", "w", encoding="utf-8") as f:
        json.dump(performance_report, f, indent=2, ensure_ascii=False)
    
    print("ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: performance_report.json")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é…ç½®ç®¡ç†å™¨æµ‹è¯•è¿è¡Œå™¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    parser.add_argument(
        "--type",
        choices=["unit", "api", "integration", "cache", "performance", "database", "redis", "stress", "async-performance", "all"],
        default="all",
        help="æµ‹è¯•ç±»å‹"
    )
    parser.add_argument(
        "--setup-only",
        action="store_true",
        help="ä»…è®¾ç½®æµ‹è¯•ç¯å¢ƒ"
    )
    parser.add_argument(
        "--cleanup-only",
        action="store_true",
        help="ä»…æ¸…ç†æµ‹è¯•ç¯å¢ƒ"
    )
    parser.add_argument(
        "--no-cleanup",
        action="store_true",
        help="ä¸æ¸…ç†æµ‹è¯•ç¯å¢ƒ"
    )
    parser.add_argument(
        "--async-test",
        action="store_true",
        help="è¿è¡Œå¼‚æ­¥æ€§èƒ½æµ‹è¯•"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ é…ç½®ç®¡ç†å™¨æµ‹è¯•è¿è¡Œå™¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    print(f"ğŸ“‹ æµ‹è¯•ç±»å‹: {args.type}")
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    if not setup_test_environment():
        print("âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
        sys.exit(1)
    
    if args.setup_only:
        print("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
        return
    
    # è¿è¡Œæµ‹è¯•
    test_success = True
    
    if args.type == "unit":
        test_success = run_unit_tests()
    elif args.type == "api":
        test_success = run_api_tests()
    elif args.type == "integration":
        test_success = run_integration_tests()
    elif args.type == "cache":
        test_success = run_cache_tests()
    elif args.type == "performance":
        test_success = run_performance_tests()
    elif args.type == "database":
        test_success = run_database_tests()
    elif args.type == "redis":
        test_success = run_redis_tests()
    elif args.type == "stress":
        test_success = run_stress_test()
    elif args.type == "async-performance":
        # è¿è¡Œå¼‚æ­¥æ€§èƒ½æµ‹è¯•
        result = asyncio.run(run_async_performance_test())
        test_success = result is not None
    elif args.type == "all":
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        tests = [
            run_unit_tests,
            run_api_tests,
            run_integration_tests,
            run_cache_tests,
            run_performance_tests,
            run_database_tests,
            run_redis_tests
        ]
        
        for test_func in tests:
            if not test_func():
                test_success = False
        
        # è¿è¡Œå¼‚æ­¥æ€§èƒ½æµ‹è¯•
        if args.async_test:
            result = asyncio.run(run_async_performance_test())
            if result is None:
                test_success = False
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_test_report()
    
    # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
    if not args.no_cleanup and not args.cleanup_only:
        cleanup_test_environment()
    
    if args.cleanup_only:
        cleanup_test_environment()
        return
    
    # è¾“å‡ºç»“æœ
    if test_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        sys.exit(0)
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main() 